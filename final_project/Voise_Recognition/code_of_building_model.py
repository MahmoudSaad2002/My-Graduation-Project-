# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

author : " Mahmoud saad "

Original file is located at
    https://colab.research.google.com/drive/1WMFD3_Oknbv8eczn07Vy6jmoYju--WjL
"""

#  2. import all library
import os, random
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers import Conv1D, MaxPooling1D, Dropout, BatchNormalization, LSTM, Dense
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt

#  3. configuration
DATA_PATH = '/content/drive/MyDrive/data_train'  # change path according to path of your files
CLASSES = ['backward', 'forward', 'left', 'right', 'stop']
MAX_LEN = 8000
N_MELS = 64

#  4.Augmentation function
def add_noise(audio):
    noise = np.random.normal(0, 0.005, audio.shape)
    return audio + noise

def time_shift(audio):
    shift = np.random.randint(-200, 200)
    return np.roll(audio, shift)

def pitch_shift(audio, sr):
    return librosa.effects.pitch_shift(audio, sr, n_steps=random.uniform(-1, 1))

def stretch_audio(audio):
    rate = random.uniform(0.8, 1.2)
    return librosa.effects.time_stretch(audio.astype(float), rate)

def change_volume(audio):
    gain = random.uniform(0.8, 1.2)
    return audio * gain

#  5. Extract Features
def extract_features(file_path, augment=False):
    audio, sr = librosa.load(file_path, sr=8000)
    audio = librosa.util.fix_length(audio, size=MAX_LEN)

    if augment:
        aug_fns = [add_noise, time_shift, lambda x: pitch_shift(x, sr), stretch_audio, change_volume]
        aug_fn = random.choice(aug_fns)
        try:
            audio = aug_fn(audio)
        except:
            pass  #   fail Augmentation

    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=N_MELS)
    mel_db = librosa.power_to_db(mel, ref=np.max)
    return mel_db.T

#  6. load data
X, y = [], []
print(" Loading dataset...")
for idx, label in enumerate(CLASSES):
    folder = os.path.join(DATA_PATH, label)
    files = os.listdir(folder)
    print(folder);
    print(files);
    for file in files:
        path = os.path.join(folder, file)
        for _ in range(2):  # Augmentation x2
            X.append(extract_features(path, augment=True))
            print(X);
            y.append(idx)
            print(y);


print(X);
print(y);

#  7. convert data
X = np.array(X)
print(X);
y = to_categorical(np.array(y), num_classes=len(CLASSES))
print(y);
#  8. Normalization
scaler = StandardScaler()
X_scaled = np.array([scaler.fit_transform(sample) for sample in X])
print(X_scaled);
#  9. divide data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

#  10. Build model
model = Sequential([
    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    BatchNormalization(), MaxPooling1D(2), Dropout(0.3),

    Conv1D(128, 3, activation='relu'),
    BatchNormalization(), MaxPooling1D(2), Dropout(0.3),

    LSTM(64), Dropout(0.3),

    Dense(64, activation='relu'), Dropout(0.3),
    Dense(len(CLASSES), activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#  11. Callbacks
callbacks = [
    EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_accuracy', patience=3, factor=0.5),
    ModelCheckpoint('/content/best_model_final.keras', save_best_only=True, monitor='val_accuracy')
]

model.summary()

#  12. training
print(" Training model...")
history = model.fit(X_train, y_train, validation_data=(X_test, y_test),
                    epochs=50, batch_size=32, callbacks=callbacks)

#  13. evaluation
loss, acc = model.evaluate(X_test, y_test)
print(f"\n Final Accuracy: {acc*100:.2f}% | Loss: {loss:.4f}")

#  14. Plotting Evaluation
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Accuracy'), plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Loss'), plt.legend()
plt.tight_layout(), plt.show()
